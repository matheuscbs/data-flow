# Data-Flow Project

## Overview

This project, named `data-flow`, is designed to perform data extraction, processing, and streaming operations, utilizing technologies such as Kafka, Spark, and MongoDB. It integrates multiple services and tools to create a robust data processing pipeline that efficiently manages and processes large volumes of data.

## Project Structure

- **app/**: Main application folder.
  - **.venv/**: Virtual environment for the project dependencies.
  - **data_flow/**: Contains the ETL scripts and data processing modules.
    - \***\*init**.py\*\*: Initialization script for the data_flow package.
    - **etl.py**: Main script for the ETL (Extract, Transform, Load) process.
  - **poetry.lock**: Lock file for Python package dependencies managed by Poetry.
  - **poetry.toml**: Configuration file for Poetry settings.
  - **pyproject.toml**: Defines project metadata and dependencies managed by Poetry.
- **logs/**: Directory for log files generated by the application.
- **.env**: Environment variables for the project configuration.
- **.flake8**: Configuration file for Python code style checks.
- **.python-version**: Specifies the Python version used in the project.
- **docker-compose.yml**: Docker Compose configuration for orchestrating the multi-container setup.
- **Dockerfile.data-flow**: Dockerfile for setting up the Python environment for the data-flow application.
- **Dockerfile.kafka-mongo-setup**: Dockerfile for preparing the Kafka and MongoDB setup environment.
- **setup-kafka-mongo.sh**: Bash script for setting up Kafka topics and configuring MongoDB connectors.

## How It Works

The `data-flow` project uses Kafka to stream data continuously, which is processed in real-time by Spark. The data is then stored or further processed in MongoDB based on the defined ETL logic within the `etl.py` script. This setup allows for high throughput and scalable data processing pipelines which are essential for handling large datasets typically used in big data and analytics solutions.

## Running the Project Locally

To run this project on your local machine, follow these steps:

1. Ensure Docker and Docker Compose are installed on your system.
2. Clone the repository to your local machine.
3. Navigate to the project directory.
4. Configure the `.env` file with the necessary environment variables.
5. Run the following command to build and start all the services:
   ```bash
   docker-compose up --build
   ```
6. Monitor the logs to ensure that all services are running correctly.

The Docker containers will include all the necessary dependencies and environment required to run the application, ensuring consistency across different development and production environments.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Author

Matheus Cardoso - Initial work and maintenance. You can contact me at matheuscbsmsn1994@gmail.com.

This README now includes comprehensive instructions on how to execute the project locally, a basic explanation of its functionality, and formal sections for the license and author information. This structure ensures that anyone new to the project can quickly get up to speed and understand both the technical and administrative aspects of the project.
