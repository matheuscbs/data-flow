# Data-Flow Project

## Overview

This project, named `data-flow`, is designed to perform data extraction, processing, and streaming operations, utilizing technologies such as Kafka, Spark, and MongoDB. It integrates multiple services and tools to create a robust data processing pipeline that efficiently manages and processes large volumes of data.

## Project Structure

- **app/**: Main application folder.
  - **.venv/**: Virtual environment for the project dependencies.
  - **data_flow/**: Contains the ETL scripts and data processing modules.
    - **\_\_init.py\_\_**: Initialization script for the data_flow package.
    - **etl.py**: Main script for the ETL (Extract, Transform, Load) process.
  - **poetry.lock**: Lock file for Python package dependencies managed by Poetry.
  - **poetry.toml**: Configuration file for Poetry settings.
  - **pyproject.toml**: Defines project metadata and dependencies managed by Poetry.
- **logs/**: Directory for log files generated by the application.
- **.env**: Environment variables for the project configuration.
- **.flake8**: Configuration file for Python code style checks.
- **.python-version**: Specifies the Python version used in the project.
- **docker-compose.yml**: Docker Compose configuration for orchestrating the multi-container setup.
- **Dockerfile.data-flow**: Dockerfile for setting up the Python environment for the data-flow application.
- **Dockerfile.kafka-mongo-setup**: Dockerfile for preparing the Kafka and MongoDB setup environment.
- **setup-kafka-mongo.sh**: Bash script for setting up Kafka topics and configuring MongoDB connectors.
- **test-e2e.sh**: Bash script for test the pipeline integration.

## How It Works

The `data-flow` project uses Kafka to stream data continuously, which is processed in real-time by Spark. The data is then stored or further processed in MongoDB based on the defined ETL logic within the `etl.py` script. This setup allows for high throughput and scalable data processing pipelines which are essential for handling large datasets typically used in big data and analytics solutions.

## Running the Project

## Prerequisites

Prerequisites
Ensure you have the following installed on your system:

- **Docker**: For containerization and running the services.
- **Docker Compose**: For orchestrating the multi-container setup.
- **Poetry**: For managing Python dependencies (you can install it with pip install poetry).

## Local Execution

1. Activate the Python Environment:

```bash
poetry env use 3.9.18  # Replace with your Python version if different
```

2. Install Dependencies:

```bash
poetry install
```

3. Run the ETL Script:

```bash
poetry run python data_flow/etl.py
```

## Docker Execution

To run this project on your local machine, follow these steps:

1. Ensure Docker and Docker Compose are installed on your system.
2. Clone the repository to your local machine.
3. Navigate to the project directory.
4. Configure the `.env` file with the necessary environment variables.
5. Modify the `docker-compose.yml` file to ensure the environment variables and network settings are correctly set up:
   - Kafka and Zookeeper configurations should match your network and resource specifications.
   - Update the MongoDB URI and Kafka configurations to align with your local or development environment needs.
   - Confirm that the ports specified do not conflict with your existing applications.
6. Run the following command to build and start all the services:
   ```bash
   docker-compose up --build
   ```
   7- Monitor the logs to ensure that all services are running correctly.

The Docker containers will include all the necessary dependencies and environment required to run the application, ensuring consistency across different development and production environments.

## Configuration Details

# Docker Compose Variables

Here are some key environment variables used in docker-compose.yml that you might need to adjust depending on your setup:

- KAFKA_BROKER_ID, KAFKA_ZOOKEEPER_CONNECT: Ensure these are consistent with your Kafka cluster configuration.
- MONGO_URI: Update this to point to your MongoDB instance, which may be local or hosted.
- CONNECT_BOOTSTRAP_SERVERS: This should point to your Kafka bootstrap server.
  **Important Ports**
- 9092 & 29092: Kafka broker ports.
- 2181: Zookeeper port.
- 27017: MongoDB port.
- 8080 & 7077: Spark master ports.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Author

Matheus Cardoso - Initial work and maintenance. You can contact me at matheuscbsmsn1994@gmail.com.

This README now includes comprehensive instructions on how to execute the project locally, a basic explanation of its functionality, and formal sections for the license and author information. This structure ensures that anyone new to the project can quickly get up to speed and understand both the technical and administrative aspects of the project.
